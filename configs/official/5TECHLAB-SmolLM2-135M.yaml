run_name: 5tech_run_1
seed: 6198
dry_run: false

wandb:
  name: ${run_name}
  project: 5techLab

model:
  d_model: 896
  n_heads: 14
  n_layers: 24
  mlp_ratio: 8
  alibi: true
  alibi_bias_max: 8.0
  flash_attention: false
  attention_dropout: 0.0
  attention_layer_norm: false
  multi_query_attention: true
  block_type: sequential
  layer_norm_type: low_precision  # if not compiling, use 'low_precision'
  activation_type: silu
  residual_dropout: 0.0
  embedding_dropout: 0.0
  max_sequence_length: 32768
  include_bias: true
  vocab_size: 151936
  embedding_size: 151936
  eos_token_id: 151643
  pad_token_id: 151643
  init_device: meta
  init_std: 0.02
  max_position_embeddings: 32768
  max_window_layers: 24
  model_type: qwen2
  num_key_value_heads: 2
  rms_norm_eps: 1e-06
  rope_theta: 1000000.0
  sliding_window: 32768
  tie_word_embeddings: true
  torch_dtype: bfloat16
  transformers_version: "4.40.1"
  use_cache: true
  use_mrope: false
  use_sliding_window: false


compile: null  # causes instability on AMD GPUs

optimizer:
  name: adamw
  learning_rate: 4.0e-4
  weight_decay: 0.1
  betas:
    - 0.9
    - 0.95
  metrics_log_interval: 10

scheduler:
  name: cosine_with_warmup
  t_warmup: 2000
  alpha_f: 0.1

tokenizer:
  identifier: tokenizers/5techtokenizer/tokenizer.json
  truncate_direction: right

save_folder: ${path.choose:${oc.env:SCRATCH_DIR,no_exist}/checkpoints,/results}/${oc.env:SLURM_JOB_ID,${run_name}}
save_overwrite: false
# Sharded checkpoints (best for restarts)
save_interval: 1000
save_num_checkpoints_to_keep: 9
# Unsharded checkpoints (for final storage)
save_interval_unsharded: 10000
save_num_unsharded_checkpoints_to_keep: -1

load_path: null

max_duration: 739_328  # 3.1T tokens
epoch: 10
global_train_batch_size: 512
device_train_microbatch_size: 8

precision: amp_bf16

fsdp:
  wrapping_strategy: null
  precision: mixed

max_grad_norm: 1.0
max_grad_norm_ratio: null

speed_monitor:
  window_size: 20

eval_interval: ${save_interval}
eval_subset_num_batches: -1
device_eval_batch_size: ${device_train_microbatch_size}
evaluators:
  # lump all the small datasets together (we still get separate metrics).
  - label: v3-small-ppl-validation
    data:
      num_workers: 0
      drop_last: true
      datasets:
        v3-small-c4_en-validation:
          - https://olmo-data.org/eval-data/perplexity/v3_small_gptneox20b/c4_en/val/part-0-00000.npy


  - label: v2-small-ppl-validation
    data:
      num_workers: 0
      drop_last: true
      datasets:
        v2-small-4chan-validation:
          - https://olmo-data.org/eval-data/perplexity/v2_small_gptneox20b/4chan/val.npy

  - label: piqa
    type: downstream

  - label: hellaswag
    type: downstream

  - label: winogrande
    type: downstream

  - label: openbook_qa
    type: downstream

    # - label: boolq  # requires implemention of the pmi_dc matrix
    # type: downstream

  - label: sciq
    type: downstream

  - label: arc_easy
    type: downstream

  # - label: arc_challenge  # requires implemention of the pmi_dc matrix
  #   type: downstream

  - label: copa
    type: downstream

  - label: rte
    type: downstream

  - label: commitment_bank
    type: downstream

  - label: mrpc
    type: downstream

  - label: sst2
    type: downstream

data:
  pad_direction: right
  num_workers: 0
  drop_last: true
  pin_memory: true
  prefetch_factor: 16
  persistent_workers: true
  timeout: 0
  paths:
    - custom_data/test.npy/0_00000.npy
